# This is a sample Python script.

# Press ⌃R to execute it or replace it with your code.
# Press Double ⇧ to search everywhere for classes, files, tool windows, actions, and settings.

# -*- coding: utf-8 -*-
"""soft vq-vae.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18gSVTUcRALAPn2OADZWF9DZciyMYd3L0

# [VQ-VAE](https://arxiv.org/abs/1711.00937) by  [Aäron van den Oord](https://twitter.com/avdnoord) et al. in PyTorch

## Introduction

Variational Auto Encoders (VAEs) can be thought of as what all but the last layer of a neural network is doing, namely feature extraction or seperating out the data. Thus given some data we can think of using a neural network for representation generation.

Recall that the goal of a generative model is to estimate the probability distribution of high dimensional data such as images, videos, audio or even text by learning the underlying structure in the data as well as the dependencies between the different elements of the data. This is very useful since we can then use this representation to generate new data with similar properties. This way we can also learn useful features from the data in an unsupervised fashion.

The VQ-VAE uses a discrete latent representation mostly because many important real-world objects are discrete. For example in images we might have categories like "Cat", "Car", etc. and it might not make sense to interpolate between these categories. Discrete representations are also easier to model since each category has a single value whereas if we had a continous latent space then we will need to normalize this density function and learn the dependencies between the different variables which could be very complex.

### Code

I have followed the code from the TensorFlow implementation by the author which you can find here [vqvae.py](https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py) and [vqvae_example.ipynb](https://github.com/deepmind/sonnet/blob/master/sonnet/examples/vqvae_example.ipynb).

Another PyTorch implementation is found at [pytorch-vqvae](https://github.com/ritheshkumar95/pytorch-vqvae).


## Basic Idea

The overall architecture is summarized in the diagram below:

![](https://github.com/zalandoresearch/pytorch-vq-vae/blob/master/images/vq-vae.png?raw=1)

We start by defining a latent embedding space of dimension `[K, D]` where `K` are the number of embeddings and `D` is the dimensionality of each latent embeddng vector, i.e. $e_i \in \mathbb{R}^{D}$. The model is comprised of an encoder and a decoder. The encoder will map the input to a sequence of discrete latent variables, whereas the decoder will try to reconstruct the input from these latent sequences.

More preciesly, the model will take in batches of RGB images,  say $x$, each of size 32x32 for our example, and pass it through a ConvNet encoder producing some output $E(x)$, where we make sure the channels are the same as the dimensionality of the latent embedding vectors. To calculate the discrete latent variable we find the nearest embedding vector and output it's index.

The input to the decoder is the embedding vector corresponding to the index which is passed through the decoder to produce the reconstructed image.

Since the nearest neighbour lookup has no real gradient in the backward pass we simply pass the gradients from the decoder to the encoder  unaltered. The intuition is that since the output representation of the encoder and the input to the decoder share the same `D` channel dimensional space, the gradients contain useful information for how the encoder has to change its output to lower the reconstruction loss.

## Loss

The total loss is actually composed of three components

1. **reconstruction loss**: which optimizes the decoder and encoder
1. **codebook loss**: due to the fact that gradients bypass the embedding, we use a dictionary learning algorithm  which uses an $l_2$  error to move the embedding vectors $e_i$ towards the encoder output
1. **commitment loss**:  since the volume of the embedding space is dimensionless, it can grow arbirtarily if the embeddings $e_i$ do not train as fast as  the encoder parameters, and thus we add a commitment loss to make sure that the encoder commits to an embedding
"""
import argparse
import matplotlib.pyplot as plt
import numpy as np
import random
from scipy.signal import savgol_filter
import math
import time
import warnings

import json
import os

import umap

import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import ParameterGrid
from torch.utils.data import DataLoader, random_split
import torch.optim as optim
import warnings
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torchvision.utils import make_grid
from cffi.backend_ctypes import xrange
from tqdm import tqdm
from vqtorch.nn import VectorQuant, GroupVectorQuant, ResidualVectorQuant

from pynndescent.optimal_transport import initialize_graph_structures

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""## Load Data"""


# Kmeans--init hook definition


@torch.no_grad()
def data_dependent_init_forward_hook(self, inputs, outputs, use_kmeans=True, verbose=False):
    """ initializes codebook from data """

    if (not self.training) or (self.data_initialized.item() == 1):
        return

    if verbose:
        print('initializing codebook with k-means++')

    def sample_centroids(z_e, num_codes):
        """ replaces the data of the codebook with z_e randomly. """

        z_e = z_e.reshape(-1, z_e.size(-1))

        if num_codes >= z_e.size(0):
            e_msg = f'\ncodebook size > warmup samples: {num_codes} vs {z_e.size(0)}. ' + \
                    'recommended to decrease the codebook size or increase batch size.'

            # warnings.warn(str(cs(e_msg, 'yellow')))

            # repeat until it fits and add noise
            repeat = num_codes // z_e.shape[0]
            new_codes = z_e.data.tile([repeat, 1])[:num_codes]
            new_codes += 1e-3 * torch.randn_like(new_codes.data)

        else:
            # you have more warmup samples than codebook. subsample data
            if use_kmeans:
                from torchpq.clustering import KMeans
                kmeans = KMeans(n_clusters=num_codes, distance='euclidean', init_mode="kmeans++")
                kmeans.fit(z_e.data.T.contiguous())
                new_codes = kmeans.centroids.T
            else:
                indices = torch.randint(low=0, high=num_codes, size=(num_codes,))
                indices = indices.to(z_e.device)
                new_codes = torch.index_select(z_e, 0, indices).to(z_e.device).data

        return new_codes

    _, z_q, _, _, z_e = outputs
    # the z_e here is the output of encoder(i.e. the input of vqvae) which has been reshaped to [b,h,w,c]

    if type(self) is VectorQuantizerEMA:
        num_codes = self._embedding.weight.shape[0]
        new_codebook = sample_centroids(z_e, num_codes)
        self._embedding.weight.data = new_codebook

    self.data_initialized.fill_(1)
    return


class SoftClustering:
    def __init__(self, delta=0.2, lr=0.0):
        """
        Initialize the SoftClustering class.

        Args:
            delta (float): The update rate (a constant).
        """
        self.delta = delta
        self.epsilon = torch.tensor(0.5)
        self.lr = lr
        self.sim_matrix = None

    def _compute_similarity(self, v1, v2):
        distances = torch.norm(v1 - v2, p=2, dim=2)
        similarity = torch.exp(1.0 / (distances + self.epsilon))
        return similarity

    def update_delta(self):
        self.delta -= self.delta * self.lr

    # @torch.no_grad()
    # def update_sim_matrix(self, codebook):
    #     """
    #     Perform soft clustering update on signal points.
    #
    #     Args:
    #         codebook (torch.Tensor): The original codebook.
    #
    #     Returns:
    #         newcodebook (torch.Tensor): The updated codebook.
    #     """
    #
    #     num_codes, feature_dim = codebook.shape
    #
    #     codebook_expanded = codebook.unsqueeze(0).expand(num_codes, -1, -1)
    #     codebook_transposed = codebook_expanded.permute(1, 0, 2)
    #     distances = torch.norm(codebook_expanded - codebook_transposed, p=2, dim=2)
    #     self.sim_matrix = torch.exp(1.0 / (distances + self.epsilon))
    #
    #     del distances, codebook_expanded, codebook_transposed
    #     # return codebook
    @torch.no_grad()
    def update_sim_matrix(self, codebook):
        """
        Perform soft clustering sim_matrix update with signal points.

        Args:
            codebook (torch.Tensor): The original codebook.

        Returns:
            new sim_matrix (torch.Tensor): The updated sim_matrix.
        """

        num_codes, feature_dim = codebook.shape

        codebook_expanded = codebook.unsqueeze(0).expand(num_codes, -1, -1)
        codebook_transposed = codebook_expanded.permute(1, 0, 2)
        distances = torch.norm(codebook_expanded - codebook_transposed, p=2, dim=2)
        self.sim_matrix = torch.exp(1.0 / (distances + self.epsilon))

        del distances, codebook_expanded, codebook_transposed
        # return codebook

    def update_codebook(self, codebook):
        # compute sim_sum, for computing weight matrix
        sim_sum = torch.sum(self.sim_matrix, dim=1)
        # expand dim of sim_sum
        sim_sum = sim_sum.unsqueeze(1)
        # compute weight matrix
        weights = self.sim_matrix / sim_sum
        # compute weighted_codebook_sum with matrix multiply
        weighted_codebook_sum = torch.matmul(weights, codebook)
        codebook = (1 - self.delta) * codebook + self.delta * weighted_codebook_sum
        del weights, weighted_codebook_sum, sim_sum
        # return codebook  # this is in-place operation so no need to return


# test_cluster = SoftClustering()
# import time
# import torch

# _num = 2048
# _dim = 64
# test_input = torch.rand([_num, _dim]).to(device)
# test_cluster.update_codebook(test_input)

"""## Vector Quantizer Layer

This layer takes a tensor to be quantized. The channel dimension will be used as the space in which to quantize. All other dimensions will be flattened and will be seen as different examples to quantize.

The output tensor will have the same shape as the input.

As an example for a `BCHW` tensor of shape `[16, 64, 32, 32]`, we will first convert it to an `BHWC` tensor of shape `[16, 32, 32, 64]` and then reshape it into `[16384, 64]` and all `16384` vectors of size `64`  will be quantized independently. In otherwords, the channels are used as the space in which to quantize. All other dimensions will be flattened and be seen as different examples to quantize, `16384` in this case.
"""


class VectorQuantizer(nn.Module):
    def __init__(
            self,
            num_embeddings,
            embedding_dim,
            commitment_cost,
            kmeans_init=True,
            soft_discretization=False,
            gamma=0.8,
            gamma_lr=1 / 10000,
            soft_clustering=False,
            delta=0.2,
            delta_lr=0,
            every_cluster_iters=250,
            delta_decrease_threshold=0,
    ):

        super(VectorQuantizer, self).__init__()

        self._embedding_dim = embedding_dim
        self._num_embeddings = num_embeddings

        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)
        self._embedding.weight.data.normal_()
        self._commitment_cost = commitment_cost

        if kmeans_init:
            self.register_buffer('data_initialized', torch.zeros(1))
            self.register_forward_hook(data_dependent_init_forward_hook)

        self.soft_discretization = soft_discretization
        self.gamma = gamma
        self.gamma_lr = gamma_lr

        self.delta = delta
        self.delta_lr = delta_lr
        self.every_cluster_iters = every_cluster_iters
        self.decrease_threshold = delta_decrease_threshold

        if soft_clustering:
            self.soft_cluster_assignment = SoftClustering(delta=self.delta, lr=self.delta_lr)

        self.training_iter_num = 0

    def forward(self, inputs):
        # convert inputs from BCHW -> BHWC
        inputs = inputs.permute(0, 2, 3, 1).contiguous()
        input_shape = inputs.shape

        # Flatten input
        flat_input = inputs.view(-1, self._embedding_dim)  # [BHW, C]

        # perform codebook update
        if self.training:
            if hasattr(self, 'soft_cluster_assignment'):
                if self.soft_cluster_assignment.delta > 0 and self.training_iter_num % self.every_cluster_iters == 0:  # 每迭代完一次所有batch更新一下codebook
                    self.soft_cluster_assignment.update_sim_matrix(self._embedding.weight)
                if self.soft_cluster_assignment.delta > 0:
                    self.soft_cluster_assignment.update_codebook(self._embedding.weight)
                if self.soft_cluster_assignment.lr > 0 and self.training_iter_num >= self.decrease_threshold:
                    self.soft_cluster_assignment.update_delta()
            self.training_iter_num += 1

        # Calculate distances
        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)
                     + torch.sum(self._embedding.weight ** 2, dim=1)
                     - 2 * torch.matmul(flat_input,
                                        self._embedding.weight.t()))  # [BHW, num_embeddings] 对于任一个连续向量，储存其和所有code的距离

        # Encoding
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)  # [BHW, 1]
        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings,
                                device=inputs.device)  # [BHW, num_embeddings]
        encodings.scatter_(1, encoding_indices, 1)  # [BHW, num_embeddings] 对于任一个连续向量，以one-hot的形式储存和其距离最小的code的指标

        # Quantize and unflatten
        quantized = torch.matmul(encodings, self._embedding.weight).view(
            input_shape)  # [B, H, W, embedding_dim] 对于任一个连续向量，储存和其距离最小的code

        # Soft discretization
        if self.training:
            if self.soft_discretization:
                if self.gamma > 0:
                    quantized = (1 - self.gamma) * quantized + self.gamma * inputs
                    self.gamma -= self.gamma * self.gamma_lr

        # Loss
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        c_loss = F.mse_loss(quantized, inputs.detach())
        loss = c_loss + self._commitment_cost * e_latent_loss

        # Straight Through Estimator
        quantized = inputs + (quantized - inputs).detach()
        avg_probs = torch.mean(encodings, dim=0)
        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + torch.tensor(1e-10))))

        # convert quantized from BHWC -> BCHW
        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings, inputs


"""We will also implement a slightly modified version  which will use exponential moving averages to update the 
embedding vectors instead of an auxillary loss. This has the advantage that the embedding updates are independent of 
the choice of optimizer for the encoder, decoder and other parts of the architecture. For most experiments the EMA 
version trains faster than the non-EMA version."""


class VectorQuantizerEMA(nn.Module):
    def __init__(
            self,
            num_embeddings,
            embedding_dim,
            commitment_cost,
            decay,
            epsilon=1e-5,
            kmeans_init=True,
            soft_discretization=False,
            gamma=0.8,
            gamma_lr=1 / 10000,
            soft_clustering=False,
            delta=0.2,
            delta_lr=0,
            every_cluster_iters=250,
            delta_decrease_threshold=0,
    ):

        super(VectorQuantizerEMA, self).__init__()

        self._embedding_dim = embedding_dim
        self._num_embeddings = num_embeddings

        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)
        self._embedding.weight.data.normal_()
        self._commitment_cost = commitment_cost

        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))  # 用来跟踪每个码字的累计分配样本数，buffer不参与训练
        self._ema_w = nn.Parameter(
            torch.Tensor(num_embeddings, self._embedding_dim))  # [_num, _dim] 是模型参数，表示在指数移动平均中使用的权重向量，参与训练
        self._ema_w.data.normal_()

        self._decay = decay
        self._epsilon = epsilon

        if kmeans_init:
            self.register_buffer('data_initialized', torch.zeros(1))
            self.register_forward_hook(data_dependent_init_forward_hook)

        self.soft_discretization = soft_discretization
        self.gamma = gamma
        self.gamma_lr = gamma_lr

        self.delta = delta
        self.delta_lr = delta_lr
        self.every_cluster_iters = every_cluster_iters
        self.decrease_threshold = delta_decrease_threshold

        if soft_clustering:
            self.soft_cluster_assignment = SoftClustering(delta=self.delta, lr=self.delta_lr)

        self.training_iter_num = 0

    def forward(self, inputs):
        # convert inputs from BCHW -> BHWC
        inputs = inputs.permute(0, 2, 3, 1).contiguous()
        input_shape = inputs.shape

        # Flatten input
        flat_input = inputs.view(-1, self._embedding_dim)  # [BHW, C]

        # perform codebook update
        if self.training:
            if hasattr(self, 'soft_cluster_assignment'):
                if self.soft_cluster_assignment.delta > 0 and self.training_iter_num % self.every_cluster_iters == 0:  # 每迭代完一次所有batch更新一下codebook
                    self.soft_cluster_assignment.update_sim_matrix(self._embedding.weight)
                    self.soft_cluster_assignment.update_codebook(self._embedding.weight)
                if self.soft_cluster_assignment.lr > 0 and self.training_iter_num >= self.decrease_threshold and \
                        self.soft_cluster_assignment.delta > 0:
                    self.soft_cluster_assignment.update_delta()
            self.training_iter_num += 1

        # Calculate distances
        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)
                     + torch.sum(self._embedding.weight ** 2, dim=1)
                     - 2 * torch.matmul(flat_input,
                                        self._embedding.weight.t()))  # [BHW, num_embeddings] 对于任一个连续向量，储存其和所有code的距离

        # Encoding
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)  # [BHW, 1]
        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings,
                                device=inputs.device)  # [BHW, num_embeddings]
        encodings.scatter_(1, encoding_indices, 1)  # [BHW, num_embeddings] 对于任一个连续向量，以one-hot的形式储存和其距离最小的code的指标

        # Quantize and unflatten
        quantized = torch.matmul(encodings, self._embedding.weight).view(
            input_shape)  # [B, H, W, embedding_dim] 对于任一个连续向量，储存和其距离最小的code

        # Soft discretization
        if self.training:
            if self.soft_discretization:
                if self.gamma > 0:
                    quantized = (1 - self.gamma) * quantized + self.gamma * inputs
                    self.gamma -= self.gamma * self.gamma_lr

        # Use EMA to update the embedding vectors
        if self.training:
            self._ema_cluster_size = self._ema_cluster_size * self._decay + \
                                     (1 - self._decay) * torch.sum(encodings, 0)  # （num_embedding, ), track the
            # assigment numbers

            # Laplace smoothing of the cluster size
            n = torch.sum(self._ema_cluster_size.data)
            self._ema_cluster_size = (
                    (self._ema_cluster_size + self._epsilon)
                    / (
                            n + self._num_embeddings * self._epsilon) * n)  # 对 _ema_cluster_size
            # 进行Laplace平滑，从而避免分配样本数为零的问题，同时也在计算码字的平均值时引入一些平滑调整

            dw = torch.matmul(encodings.t(), flat_input)  # (BHW, num).t .* (BHW, dim) = (num, dim),
            # 这一步是对每一个聚类加权平均，dw每一行代表一个聚类，行向量代表属于该聚类的flat_input中的sample加和的结果
            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)  # EMA更新codebook，第二个term
            # 使得codebook向encoder representation的方向更新

            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))  # 进行加权平均

        # Loss
        # Here we use quantized.detach() to make loss has no relationship with codebook, so codebook would not be
        # updated during bp
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        c_latent_loss = F.mse_loss(inputs.detach(), quantized)
        loss = self._commitment_cost * e_latent_loss

        # Straight Through Estimator
        quantized = inputs + (quantized - inputs).detach()
        avg_probs = torch.mean(encodings, dim=0)
        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + torch.tensor(1e-10))))

        # convert quantized from BHWC -> BCHW
        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings, inputs


# import torch
# N = 8 * 8 * 192
# C = 1024

# encodings = torch.zeros([N, C])
# for i in range(N):
#     column_ind = torch.randint(0, C, (1,))
#     encodings[i, column_ind] = 1
# avg_probs = torch.mean(encodings, dim=0)
# perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))
# print(perplexity)

"""## Encoder & Decoder Architecture

The encoder and decoder architecture is based on a ResNet and is implemented below:
"""


class Residual(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):
        super(Residual, self).__init__()
        self._block = nn.Sequential(
            nn.ReLU(True),
            nn.Conv2d(in_channels=in_channels,
                      out_channels=num_residual_hiddens,
                      kernel_size=3, stride=1, padding=1, bias=False),
            nn.ReLU(True),
            nn.Conv2d(in_channels=num_residual_hiddens,
                      out_channels=num_hiddens,
                      kernel_size=1, stride=1, bias=False)
        )

    def forward(self, x):
        return x + self._block(x)


class ResidualStack(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):
        super(ResidualStack, self).__init__()
        self._num_residual_layers = num_residual_layers
        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)
                                      for _ in range(self._num_residual_layers)])

    def forward(self, x):
        for i in range(self._num_residual_layers):
            x = self._layers[i](x)  # in_channels == num_hiddens
        return F.relu(x)


class Encoder(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):
        super(Encoder, self).__init__()

        self._conv = nn.Conv2d(in_channels=in_channels,
                               out_channels=num_hiddens // 2,
                               kernel_size=4,
                               stride=2, padding=1)
        self._conv_2 = nn.Conv2d(in_channels=num_hiddens // 2,
                                 out_channels=num_hiddens,
                                 kernel_size=4,
                                 stride=2, padding=1)
        self._conv_3 = nn.Conv2d(in_channels=num_hiddens,
                                 out_channels=num_hiddens,
                                 kernel_size=3,
                                 stride=1,
                                 padding=1)  # (3,1,1)尺寸保持不变, (4,2,1)对尺寸进行缩小，conv和convtranspose使用相同的(k,s,p)以恢复图像尺寸
        self._residual_stack = ResidualStack(in_channels=num_hiddens,
                                             num_hiddens=num_hiddens,
                                             num_residual_layers=num_residual_layers,
                                             num_residual_hiddens=num_residual_hiddens)

    def forward(self, inputs):
        x = self._conv(inputs)
        x = F.relu(x)

        x = self._conv_2(x)
        x = F.relu(x)

        x = self._conv_3(x)
        return self._residual_stack(x)


class Decoder(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):
        super(Decoder, self).__init__()

        self._conv = nn.Conv2d(in_channels=in_channels,
                               out_channels=num_hiddens,
                               kernel_size=3,
                               stride=1, padding=1)

        self._residual_stack = ResidualStack(in_channels=num_hiddens,
                                             num_hiddens=num_hiddens,
                                             num_residual_layers=num_residual_layers,
                                             num_residual_hiddens=num_residual_hiddens)

        self._conv_trans = nn.ConvTranspose2d(in_channels=num_hiddens,
                                              out_channels=num_hiddens // 2,
                                              kernel_size=4,
                                              stride=2, padding=1)

        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens // 2,
                                                out_channels=3,
                                                kernel_size=4,
                                                stride=2, padding=1)

    def forward(self, inputs):
        x = self._conv(inputs)

        x = self._residual_stack(x)

        x = self._conv_trans(x)
        x = F.relu(x)

        return self._conv_trans_2(x)


def show(img):
    npimg = img.numpy()
    fig = plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')
    fig.axes.get_xaxis().set_visible(False)
    fig.axes.get_yaxis().set_visible(False)


"""## Train

We use the hyperparameters from the author's code:
"""

batch_size = 200
num_training_updates = 15000

num_hiddens = 128
num_residual_hiddens = 32
num_residual_layers = 2

embedding_dim = 256
num_embeddings = 3072

commitment_cost = 0.25

decay = 0.99

learning_rate = 2e-4
checkpoint_dir = '/root/VQ-VAE/checkpoints/'


class Model(nn.Module):
    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,
                 num_embeddings, embedding_dim, commitment_cost, decay=0.0, **vq_kwargs):
        super(Model, self).__init__()

        self._encoder = Encoder(3, num_hiddens,
                                num_residual_layers,
                                num_residual_hiddens)
        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens,
                                      out_channels=embedding_dim,
                                      kernel_size=1,
                                      stride=1)
        self._vq_vae = VectorQuant(
                feature_size=embedding_dim,     # feature dimension corresponding to the vectors
                num_codes=num_embeddings,      # number of codebook vectors
                beta=commitment_cost,           # (default: 0.9) commitment trade-off
                kmeans_init=True,    # (default: False) whether to use kmeans++ init
                norm=None,           # (default: None) normalization for input vector
                cb_norm=None,        # (default: None) normalization for codebook vectors
                affine_lr=10.0,      # (default: 0.0) lr scale for affine parameters
                affine_groups=8,     # *** NEW *** (default: 1) number of affine parameter groups
                sync_nu=0.2,         # (default: 0.0) codebook syncronization contribution
                replace_freq=20,     # (default: None) frequency to replace dead codes
                dim=-1,              # (default: -1) dimension to be quantized
                )
        self._decoder = Decoder(embedding_dim,
                                num_hiddens,
                                num_residual_layers,
                                num_residual_hiddens)

    def forward(self, x):
        z = self._encoder(x)
        z = self._pre_vq_conv(z)
        quantized, to_return = self._vq_vae(z)
        vq_loss = to_return['loss']
        perplexity = to_return['perplexity']
        x_recon = self._decoder(quantized)

        return vq_loss, x_recon, perplexity, encodings


# training begins here


class Trainer:
    def __init__(self, using_ema=True, **vq_kwargs):
        if using_ema:
            self.model = Model(num_hiddens, num_residual_layers, num_residual_hiddens,
                               num_embeddings, embedding_dim,
                               commitment_cost, decay, **vq_kwargs)
            # self.model = nn.DataParallel(self.model)
            self.model.to(device)

            self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate,
                                        amsgrad=False)  # 通过对梯度平方的指数移动平均采取一些调整，AMSGrad试图避免梯度更新过于剧烈，从而更稳定地进行优化
        else:
            self.model = Model(num_hiddens, num_residual_layers, num_residual_hiddens,
                               num_embeddings, embedding_dim,
                               commitment_cost, decay=0.0, **vq_kwargs)
            # self.model = nn.DataParallel(self.model)
            self.model.to(device)

            self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate,
                                        amsgrad=False)  # 通过对梯度平方的指数移动平均采取一些调整，AMSGrad试图避免梯度更新过于剧烈，从而更稳定地进行优化
        self.test_batch_num = 10

    def train(self, save_folder, filename):

        self.model.train()
        train_res_recon_error = []
        train_res_perplexity = []
        train_res_vq_loss = []
        train_res_usage_rate = []

        for i in tqdm(xrange(num_training_updates)):
            (data, _) = next(iter(training_loader))
            data = data.to(device)
            self.optimizer.zero_grad()

            vq_loss, data_recon, perplexity, encodings = self.model(data)
            recon_error = F.mse_loss(data_recon, data) / data_variance
            usage_rate = encodings.unique(dim=0).numel() / (num_embeddings * num_embeddings) * 100

            loss = recon_error + vq_loss
            loss.backward()

            self.optimizer.step()

            train_res_recon_error.append(recon_error.item())
            train_res_perplexity.append(perplexity.item())
            train_res_vq_loss.append(vq_loss.item())
            train_res_usage_rate.append(usage_rate)

            # if (i + 1) % 500 == 0:
            #     print('%d iterations' % (i + 1))
            #     print('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))
            #     print('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))
            #     print('vq_loss: %.3f' % np.mean(train_res_vq_loss[-100:]))
            #     print('active code percent: %.3f' % np.mean(train_res_usage_rate[-100:]) + '%')
            #     print()

        """## Save Results"""

        train_res_recon_error_smooth = savgol_filter(train_res_recon_error, 201,
                                                     7)  # 进行 Savitzky-Golay 平滑滤波，以获得一个更平滑的版本，从而有助于观察数据的趋势和模式，同时去除噪声
        train_res_perplexity_smooth = savgol_filter(train_res_perplexity, 201, 7)
        train_res_vq_loss_smooth = savgol_filter(train_res_vq_loss, 201, 7)
        train_res_usage_rate_smooth = savgol_filter(train_res_usage_rate, 201, 7)

        # 保存张量到 JSON 文件
        data = {'recon_error': train_res_recon_error_smooth.tolist(),
                'perplexity': train_res_perplexity_smooth.tolist(),
                'usage_rate': train_res_usage_rate_smooth.tolist(),
                'vq_loss': train_res_vq_loss_smooth.tolist()}
        data_file = os.path.join(save_folder, filename)

        with open(data_file, 'w') as json_file:
            json.dump(data, json_file)

        return data
        # checkpoint_path = f'{checkpoint_dir}model_{num_training_updates}_{num_embeddings}_{batch_size}.pth'
        # torch.save({
        #             'epoch': i+1,
        #             'model_state_dict': model.state_dict(),
        #             'optimizer_state_dict': optimizer1.state_dict(),
        #             'loss': loss,
        #             'perplexity': perplexity,
        #         }, checkpoint_path)

    def valid(self):
        self.model.eval()

        (valid_originals, _) = next(iter(validation_loader))
        valid_originals = valid_originals.to(device)

        vq_output_eval = self.model._pre_vq_conv(self.model._encoder(valid_originals))
        _, valid_quantize, _, _, _ = self.model._vq_vae(vq_output_eval)
        valid_reconstructions = self.model._decoder(valid_quantize)

        (train_originals, _) = next(iter(training_loader))
        train_originals = train_originals.to(device)
        _, train_reconstructions, _, _, _ = self.model._vq_vae(train_originals)

        # show(make_grid(valid_reconstructions.cpu().data) + 0.5, )

        # show(make_grid(valid_originals.cpu() + 0.5))

        """## View Embedding"""

        proj = umap.UMAP(n_neighbors=3,
                         min_dist=0.1,
                         metric='cosine').fit_transform(self.model._vq_vae._embedding.weight.data.cpu())

        plt.scatter(proj[:, 0], proj[:, 1], alpha=0.3)

    def evaluate_on_valid_set(self):
        self.model.eval()
        average_usage_rate = 0.0
        average_recon_error = 0.0
        average_vq_loss = 0.0
        for i in xrange(self.test_batch_num):
            (valid_originals, _) = next(iter(validation_loader))
            valid_originals = valid_originals.to(device)

            vq_loss, data_recon, perplexity, encodings = self.model(valid_originals)

            recon_error = F.mse_loss(data_recon, valid_originals) / data_variance
            usage_rate = encodings.unique(dim=0).numel() / (num_embeddings * num_embeddings) * 100

            average_usage_rate += usage_rate
            average_vq_loss += vq_loss.item()
            average_recon_error += recon_error.item()
        return round(average_vq_loss / self.test_batch_num, 2), round(average_recon_error / self.test_batch_num, 2),\
            round(average_usage_rate / self.test_batch_num, 2)

    def evaluate_on_train_set(self):
        self.model.eval()
        average_score = 0.0
        test_batch_num = batch_size
        for i in xrange(test_batch_num):
            (train_originals, _) = next(iter(training_loader))
            train_originals = train_originals.to(device)

            vq_loss, data_recon, perplexity, encodings = self.model(train_originals)

            recon_error = F.mse_loss(data_recon, train_originals) / data_variance
            usage_rate = encodings.unique(dim=0).numel() / (num_embeddings * num_embeddings) * 100

            average_score += usage_rate
        return round(average_score / test_batch_num, 2)


if __name__ == '__main__':
    params = {
        'gamma': 0.2,
        'gamma_lr': 0.0002,
        'delta': 0.1,
        'delta_lr': 0.0,
        'delta_decrease_threshold': 2000,
        'every_cluster_iters': 1
    }
    gamma = params['gamma']
    gamma_lr = params['gamma_lr']
    delta = params['delta']
    delta_lr = params['delta_lr']
    delta_decrease_threshold = params['delta_decrease_threshold']
    every_cluster_iters = params['every_cluster_iters']

    train_dataset = datasets.CIFAR100(root="/root/autodl-tmp/cifar-100", train=True, download=True,
                                      transform=transforms.Compose([
                                         transforms.ToTensor(),
                                         transforms.Normalize((0.5, 0.5, 0.5), (1.0, 1.0, 1.0))
                                     ]))

    validation_data = datasets.CIFAR100(root="/root/autodl-tmp/cifar-100", train=False, download=True,
                                        transform=transforms.Compose([
                                           transforms.ToTensor(),
                                           transforms.Normalize((0.5, 0.5, 0.5), (1.0, 1.0, 1.0))
                                       ]))

    data_variance = np.var(train_dataset.data / 255.0)

    training_loader = DataLoader(train_dataset,
                                 batch_size=batch_size,
                                 shuffle=True,
                                 pin_memory=True)  # pin_memory=True加速数据从主内存到GPU内存的传输过程

    validation_loader = DataLoader(validation_data,
                                   batch_size=batch_size,
                                   shuffle=True,
                                   pin_memory=True)
    root = f'./{batch_size}_{num_training_updates}_{num_embeddings}_{embedding_dim}_only_ema/'
    if not os.path.exists(root):
        os.makedirs(root)

    file_list = []
    train_results_root = os.path.join(root, 'train_result/')
    model_pth_root = os.path.join(root, 'model_pth/')
    if not os.path.exists(train_results_root):
        os.makedirs(train_results_root)
    if not os.path.exists(model_pth_root):
        os.makedirs(model_pth_root)

    # create instance and train

    file_name = 'baseline'  # actually vq-vae with ema
    if os.path.isfile(os.path.join(train_results_root, file_name + '.json')):
        # load
        trainer = Trainer()
        trainer.model.load_state_dict(torch.load(os.path.join(model_pth_root, file_name + '.pth')))
        # evaluate
        valid_score = trainer.evaluate_on_valid_set()
        print(f'{file_name}: valid score:{valid_score}')
    else:
        trainer = Trainer()
        trainer.train(save_folder=train_results_root, filename=file_name + '.json')
        # evaluate
        valid_score = trainer.evaluate_on_valid_set()
        print(f'{file_name}: valid score:{valid_score}')
        # save model
        model_path = os.path.join(model_pth_root, file_name + '.pth')
        torch.save(trainer.model.state_dict(), model_path)
    file_list.append(os.path.join(train_results_root, file_name + '.json'))

    # sc + sd
    # trainer
    file_name = f'soft_discretization_{gamma}_{gamma_lr}'
    if os.path.isfile(os.path.join(train_results_root, file_name + '.json')):
        # load
        trainer = Trainer()
        trainer.model.load_state_dict(torch.load(os.path.join(model_pth_root, file_name + '.pth')))
        # evaluate
        valid_score = trainer.evaluate_on_valid_set()
        print(f'{file_name}: valid score:{valid_score}')
    else:
        trainer = Trainer(soft_discretization=True, gamma=gamma, gamma_lr=gamma_lr,
                          soft_clustering=False, delta=delta, delta_lr=delta_lr,
                          delta_decrease_threshold=delta_decrease_threshold,
                          every_cluster_iters=every_cluster_iters)
        trainer.train(save_folder=train_results_root, filename=file_name + '.json')
        # evaluate
        valid_score = trainer.evaluate_on_valid_set()
        print(f'{file_name}: valid score:{valid_score}')
        # save model
        model_path = os.path.join(model_pth_root, file_name + '.pth')
        torch.save(trainer.model.state_dict(), model_path)
    file_list.append(os.path.join(train_results_root, file_name + '.json'))

    file_name = f'soft_cluster_{delta}_{delta_lr}_{delta_decrease_threshold}_{every_cluster_iters}'
    if os.path.isfile(os.path.join(train_results_root, file_name + '.json')):
        # load
        trainer = Trainer()
        trainer.model.load_state_dict(torch.load(os.path.join(model_pth_root, file_name + '.pth')))
        # evaluate
        valid_score = trainer.evaluate_on_valid_set()
        print(f'{file_name}: valid score:{valid_score}')
    else:
        trainer = Trainer(soft_discretization=False, gamma=gamma, gamma_lr=gamma_lr,
                          soft_clustering=True, delta=delta, delta_lr=delta_lr,
                          delta_decrease_threshold=delta_decrease_threshold,
                          every_cluster_iters=every_cluster_iters)
        trainer.train(save_folder=train_results_root, filename=file_name + '.json')
        # evaluate
        valid_score = trainer.evaluate_on_valid_set()
        print(f'{file_name}: valid score:{valid_score}')
        # save model
        model_path = os.path.join(model_pth_root, file_name + '.pth')
        torch.save(trainer.model.state_dict(), model_path)
    file_list.append(os.path.join(train_results_root, file_name + '.json'))

    file_name = f'combine_sd_{gamma}_{gamma_lr}_sc_{delta}_{delta_lr}_{delta_decrease_threshold}_{every_cluster_iters}'
    if os.path.isfile(os.path.join(train_results_root, file_name + '.json')):
        # load
        trainer = Trainer()
        trainer.model.load_state_dict(torch.load(os.path.join(model_pth_root, file_name + '.pth')))
        # evaluate
        valid_score = trainer.evaluate_on_valid_set()
        print(f'{file_name}: valid score:{valid_score}')
    else:
        trainer = Trainer(soft_discretization=True, gamma=gamma, gamma_lr=gamma_lr,
                          soft_clustering=True, delta=delta, delta_lr=delta_lr,
                          delta_decrease_threshold=delta_decrease_threshold,
                          every_cluster_iters=every_cluster_iters)
        trainer.train(save_folder=train_results_root, filename=file_name + '.json')
        # evaluate
        valid_score = trainer.evaluate_on_valid_set()
        print(f'{file_name}: valid score:{valid_score}')
        # save model
        model_path = os.path.join(model_pth_root, file_name + '.pth')
        torch.save(trainer.model.state_dict(), model_path)
    file_list.append(os.path.join(train_results_root, file_name + '.json'))

    file_name = f'original'
    if os.path.isfile(os.path.join(train_results_root, file_name + '.json')):
        # load
        trainer = Trainer(using_ema=False)
        trainer.model.load_state_dict(torch.load(os.path.join(model_pth_root, file_name + '.pth')))
        # evaluate
        valid_score = trainer.evaluate_on_valid_set()
        print(f'{file_name}: valid score:{valid_score}')
    else:
        trainer = Trainer(using_ema=False)
        trainer.train(save_folder=train_results_root, filename=file_name + '.json')
        # evaluate
        valid_score = trainer.evaluate_on_valid_set()
        print(f'{file_name}: valid score:{valid_score}')
        # save model
        model_path = os.path.join(model_pth_root, file_name + '.pth')
        torch.save(trainer.model.state_dict(), model_path)
    file_list.append(os.path.join(train_results_root, file_name + '.json'))
    # visualize
    # visualize(file_list, None)

    # finish--shut down
    # os.system('/usr/bin/shutdown')
